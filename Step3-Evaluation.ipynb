{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b8ce84-8f95-4f4c-b7d4-5db5ec069bd0",
   "metadata": {},
   "source": [
    "### **Step3-Evaluation: Benchmarking and LLM-as-a-Judge**\n",
    "\n",
    "#### **1. Setup Evaluation Environment**\n",
    "Configures parameters for evaluation (e.g., GPU ID, models to compare, and data sources).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a25959-2431-4178-8865-c26c7fcb2a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-03 18:36:28,556\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "#Setup inference and evaluation parameters\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "GPU_ID=\"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU_ID\n",
    "#Cloudera Customer =0 means run the Cloudera questions and format and Customer=1 Runs the customer format and questions\n",
    "Customer=0\n",
    "CustomPrompt=1\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "import Libs.DataSynthesisLib as DataSynthesisLib\n",
    "Config=DataSynthesisLib.GetConfig('Config/Config.py')\n",
    "ModelBase=\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#How many lines to trim of the output\n",
    "StartLineFT=1\n",
    "StartLineBase=3\n",
    "\n",
    "#Models to use\n",
    "ModelFT='./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05'\n",
    "\n",
    "\n",
    "EvalLLM='microsoft/phi-4'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820868ec-c57c-4716-843c-d68ea966cd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Model length used 32000\n"
     ]
    }
   ],
   "source": [
    "# Detect specify the model length. Use the smaller of maximum given by the user (ModelLen) and model max length\n",
    "ModelLen=32000 #Specify max model len. Make sure this value is less than the maximum supported by the model.\n",
    "\n",
    "\n",
    "from transformers import AutoConfig\n",
    "ModelConfig = AutoConfig.from_pretrained(ModelFT)\n",
    "fields = [\"max_position_embeddings\", \"n_positions\", \"seq_len\", \"seq_length\", \"n_ctx\", \"sliding_window\"] \n",
    "context_windows = [getattr(ModelConfig, field) for field in fields if field in dir(ModelConfig)]\n",
    "ModelLenTemp=context_windows.pop() if len(context_windows) else ModelLen\n",
    "if ModelLenTemp<ModelLen:\n",
    "    ModelLen=ModelLenTemp\n",
    "    \n",
    "\n",
    "#print(ModelConfig)\n",
    "print(f\"Maximum Model length used {ModelLen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eda16f9-94bb-40ff-9158-1af275b28aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test if the LLM needs system prompt\n",
    "HasSystem=False\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ModelFT)\n",
    "    chat = [{\"role\": \"system\", \"content\": \"\"}]\n",
    "\n",
    "    tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    HasSystem=True\n",
    "except:\n",
    "    HasSystem=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ecab5c-3125-4c78-a91e-613b88dc453c",
   "metadata": {},
   "source": [
    "#### **2. Prepare Evaluation data**\n",
    "- Loads evaluation dataset for Customer or Cloudera comments depending on the target evaluation.\n",
    "- Extracts the customer questions used for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851eca19-e15c-4621-a9b5-03fd195bd607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load appropriate evaluation dataset\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "if Customer==1:\n",
    "  with open('Data/CustomerComments_Evaluation_Clean.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "elif Customer==0:\n",
    "  with open('Data/ClouderaComments_Evaluation_Clean.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d6ca99-405d-4a7b-8d49-62f92918d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "#Setup questions for LLM-as-a-judge, collect prompts for inference, and customer or Cloudera comments to be used later for evaluation\n",
    "import vllm\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(ModelFT)\n",
    "\n",
    "System='You are a helpful assistant.'\n",
    "\n",
    "\n",
    "FixedQuestions={}\n",
    "if Customer==1:\n",
    "  FixedQuestions[1]=\"Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[2]=\"Does this comment relate to a customer complaint? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[3]=\"Customer complaint temperature or a frustration level (if there is a complain give a Score 1-4, lowest=1, highest=4. If there is no complain give a score of 0)\"\n",
    "  FixedQuestions[4]=\"Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[5]=\"Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[6]=\"Is this a request from a customer for an update? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[7]=\"Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. (BOOL)\"\n",
    "  FixedQuestions[8]=\"Did the customer request an escalation? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[9]=\"Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by 4). If there is no priority change give a score of 0).\"\n",
    "  FixedQuestions[10]=\"Did the customer request a transfer to another COE? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[11]=\"Did the customer request to speak to a manager or supervisor? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[12]=\"Did the customer request a SME or expert? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[13]=\"Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[14]=\"Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[15]=\"Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[16]=\"Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[17]=\"Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]\"\n",
    "elif Customer==0:\n",
    "  FixedQuestions[1]=\"Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[2]=\"Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[3]=\"Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)\"\n",
    "  FixedQuestions[4]=\"Does the comment have a proposed solution? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[5]=\"Does the comment have a proposed workaround?  (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[6]=\"Does the comment have a request for an action from the customer?  (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[7]=\"Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[8]=\"Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[9]=\"Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[10]=\"Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)\"\n",
    "  FixedQuestions[11]=\"Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT] \"\n",
    "\n",
    "\n",
    "Questions=[]\n",
    "Comment=[]\n",
    "BaselineSolutions=[]\n",
    "for i in data:\n",
    "  if Customer==1:\n",
    "      \n",
    "    if CustomPrompt==1:\n",
    "      Questions.append(i['Prompt'])\n",
    "      Comment.append(i['Comment'])\n",
    "\n",
    "  elif Customer==0:\n",
    "    if CustomPrompt==1:\n",
    "      Questions.append(i['Prompt'])\n",
    "      Comment.append(i['Comment'])\n",
    "\n",
    "\n",
    "print(len(Questions))\n",
    "lens=[len(i) for i in Questions ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6e514-8fe8-47d1-8918-19bdf4678d08",
   "metadata": {},
   "source": [
    "#### **3. Prepare Prompts and Baseline Outputs**\n",
    "Loads existing baseline outputs (from the unfinetuned model) and prepares prompts for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe3974a-8a84-40df-8ce5-435d036f6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  BaseModelName='Base'\n",
    "\n",
    "  import  pickle\n",
    "  if Customer==1:\n",
    "    with open('Data/Eval_Customer_Questions_Solutions_'+BaseModelName+'.pickle', 'rb') as handle:\n",
    "      Output=pickle.load( handle)\n",
    "  elif Customer==0:\n",
    "    with open('Data/Eval_Cloudera_Questions_Solutions_'+BaseModelName+'.pickle', 'rb') as handle:\n",
    "      Output=pickle.load( handle)\n",
    "\n",
    "\n",
    "  SolutionsBase=Output[\"S\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b690f09e-6e29-481c-b367-a41f9ef2d61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a Cloudera support team comment, 11 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the Cloudera comment:\n",
      "    \n",
      "Hi Team,\n",
      "\n",
      "We're experiencing issues after upgrading to AD 2019 where LDAP group synchronization is failing. The error in Cloudera Manager shows:\n",
      "\n",
      "LDAP Result Code 32 (No Such Object): Failed to retrieve LDAP groups\n",
      "\n",
      "We haven't changed any LDAP configuration in Cloudera Manager. Our current settings are:\n",
      "- LDAP URL: ldaps://ad.company.local:636\n",
      "- Base DN: dc=company,dc=local\n",
      "- Group search base: ou=Groups,dc=company,dc=local\n",
      "\n",
      "Could you please help identify what might have changed and how to resolve this?\n",
      "\n",
      "Best regards,\n",
      "John\n",
      "\n",
      "Here are the 11 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "3. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "4. Does the comment have a proposed solution? (answer 0 for no, 1 for yes)\n",
      "5. Does the comment have a proposed workaround?  (answer 0 for no, 1 for yes)\n",
      "6. Does the comment have a request for an action from the customer?  (answer 0 for no, 1 for yes)\n",
      "7. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "8. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "9. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "10. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "11. Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]    \n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(Questions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22dcc5f-184e-4eac-b905-b01c083e660b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1. 1\n",
      "2. 1\n",
      "3. 1\n",
      "4. 0\n",
      "5. 0\n",
      "6. 0\n",
      "7. 0\n",
      "8. 0\n",
      "9. 0\n",
      "10. 0\n",
      "11. The customer is inquiring about compatibility between CDP 7.1.7 and Amazon OpenSearch 2.7 for integration, specifically for log analytics and searching capabilities.\n"
     ]
    }
   ],
   "source": [
    "print(SolutionsBase[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce040303-f3f1-4a84-b8cc-834045066606",
   "metadata": {},
   "source": [
    "#### **4. Generate Finetuned Model Outputs**\n",
    "Runs inference on the evaluation dataset using the finetuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4f97e1c-98b6-461a-b157-48888cf06427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05\n",
      "WARNING 04-03 18:36:57 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 04-03 18:36:57 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05', speculative_config=None, tokenizer='./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 04-03 18:36:58 model_runner.py:1060] Starting to load model ./tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.49s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:10<00:09,  4.83s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.01s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.22s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.05s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-03 18:37:19 model_runner.py:1071] Loading model weights took 14.9888 GB\n",
      "INFO 04-03 18:37:22 gpu_executor.py:122] # GPU blocks: 6230, # CPU blocks: 0\n",
      "INFO 04-03 18:37:22 gpu_executor.py:126] Maximum concurrency for 32000 tokens per request: 3.12x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [01:47<00:00,  4.65it/s, est. speed input: 2554.23 toks/s, output: 469.44 toks/s]\n"
     ]
    }
   ],
   "source": [
    "#Finetuned model parameters setup and inference \n",
    "\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "from transformers import AutoConfig\n",
    "\n",
    "try:\n",
    "     set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "ConfigS=Config[\"ConfigSolution\"].copy()\n",
    "ConfigS[\"gpu-id\"]=GPU_ID\n",
    "ConfigS[\"tensor_parallel_size\"]=\"1\"\n",
    "ConfigS[\"llm-path\"]=ModelFT\n",
    "ConfigS[\"tokenizer-path\"]=ModelFT\n",
    "ConfigS[\"system\"]=System\n",
    "ConfigS[\"use-grammar\"]=\"False\"\n",
    "ConfigS[\"AssistantPrompt\"]=\"\"\n",
    "\n",
    "ConfigS[\"max_tokens\"]=\"3000\"\n",
    "ConfigS[\"temperature\"]=\"0.1\"\n",
    "ConfigS[\"max_num_seqs\"]=\"2\"\n",
    "\n",
    "ConfigS[\"max_num_seqs\"]=\"16\"\n",
    "ConfigS[\"max_model_len\"]=str(ModelLen)\n",
    "ConfigS[\"max_num_batched_tokens\"]=str(ModelLen)\n",
    "\n",
    "\n",
    "ConfigS[\"AssistantPromptElementsRemoval\"]=\"0\"\n",
    "\n",
    "\n",
    "\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "ConfigS[\"temperature\"]=\"0\"\n",
    "if HasSystem==False:\n",
    "  ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithoutSystemWithAssistantStart\"\n",
    "else:\n",
    "  ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithSystemWithAssistantStart\"\n",
    "\n",
    "ConfigS[\"UseOutlines\"]=\"False\"\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "if 'gptq' in ModelFT:\n",
    "  ConfigS[\"quantization\"]=\"gptq\"\n",
    "ConfigS[\"gpu_memory_utilization\"]=\"0.7\"\n",
    "\n",
    "\n",
    "(Solutions, SolutionsLogging)=DataSynthesisLib.GetSolutions(ConfigS, Questions, ConfigS[\"system\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e2c6152-dc6d-4e76-a37a-61dda74bd362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a Cloudera support team comment, 11 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the Cloudera comment:\n",
      "    \n",
      "Hi Team,\n",
      "\n",
      "I've completed a security audit of your cloud workloads and identified critical vulnerabilities in your CSPM implementation:\n",
      "\n",
      "1. Unprotected API endpoints detected\n",
      "2. Missing logging and monitoring configurations\n",
      "3. Inadequate cloud asset inventory management\n",
      "\n",
      "To address these security gaps, please implement the following measures urgently:\n",
      "\n",
      "- Enable API Gateway protection mechanisms\n",
      "- Configure comprehensive logging and monitoring\n",
      "- Implement automated asset discovery and tracking\n",
      "\n",
      "Please provide an update once these security controls are in place for verification.\n",
      "\n",
      "Best regards,\n",
      "David Chen\n",
      "Principal Security Engineer\n",
      "Cloudera Support\n",
      "\n",
      "Here are the 11 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "3. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "4. Does the comment have a proposed solution? (answer 0 for no, 1 for yes)\n",
      "5. Does the comment have a proposed workaround?  (answer 0 for no, 1 for yes)\n",
      "6. Does the comment have a request for an action from the customer?  (answer 0 for no, 1 for yes)\n",
      "7. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "8. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "9. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "10. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "11. Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]    \n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Questions[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78a33c1-f752-4eae-b75a-5f5a328895bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1\n",
      "2. 4\n",
      "3. 4\n",
      "4. 1\n",
      "5. 0\n",
      "6. 1\n",
      "7. 0\n",
      "8. 0\n",
      "9. 0\n",
      "10. 0\n",
      "11. Critical security vulnerabilities identified in CSPM implementation including unprotected API endpoints, missing logging/monitoring, and inadequate asset management. Customer requested to implement API protection, logging/monitoring, and asset tracking mechanisms.\n"
     ]
    }
   ],
   "source": [
    "#Example finetuned model output\n",
    "print(Solutions[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b361ef07-647b-45ea-b16e-4c596b11df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1. 1\n",
      "2. 4\n",
      "3. 4\n",
      "4. 0\n",
      "5. 0\n",
      "6. 0\n",
      "7. 1\n",
      "8. 0\n",
      "9. 0\n",
      "10. 0\n",
      "11. The comment discusses performance degradation and timeout errors when joining tables between two CDW Virtual Warehouses in different environments, with an average query runtime of 25+ minutes and a timeout error after 1800 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Example finetuned model output\n",
    "print(SolutionsBase[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d33f85-fa4e-4fdc-908a-82978556328d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)',\n",
       " 2: 'Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)',\n",
       " 3: 'Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)',\n",
       " 4: 'Does the comment have a proposed solution? (BOOL: 0 for no, 1 for yes)',\n",
       " 5: 'Does the comment have a proposed workaround?  (BOOL: 0 for no, 1 for yes)',\n",
       " 6: 'Does the comment have a request for an action from the customer?  (BOOL: 0 for no, 1 for yes)',\n",
       " 7: 'Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)',\n",
       " 8: 'Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)',\n",
       " 9: 'Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)',\n",
       " 10: 'Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)',\n",
       " 11: 'Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT] '}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Individual questions for evaluating each question using LLM-as-a-judge\n",
    "FixedQuestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223e6ae-0ad8-4754-af23-416476ff8e51",
   "metadata": {},
   "source": [
    "#### **5. Parse Outputs and Validate Format for both Base and finetune model outputs**\n",
    "Ensures the model outputs adhere to the expected structure (e.g., numbered answers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb086ce-ca2f-42d3-97d4-8fa3b526c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse LLM output and extract analytics and summary\n",
    "def FormatOutput(Solutions,StartLine=3,ExpectedQuestions=17):\n",
    "  CorrectFormat=0\n",
    "  Output=[]\n",
    "  Correct=[]\n",
    "\n",
    "  for d in Solutions:\n",
    "    result=d.split('\\n')\n",
    "    QuestionsAnswers={}\n",
    "    count=0\n",
    "    CorrectFormat=0\n",
    "      \n",
    "    for j in result[StartLine-1:]:\n",
    "      FoundNumber=0\n",
    "      count+=1\n",
    "      context=\"\"\n",
    "      number=j.split('.')[0]\n",
    "      if number.isdigit():\n",
    "          number=int(number)\n",
    "          if number==count:\n",
    "              FoundNumber=1\n",
    "      tmp=j.split(' ')\n",
    "      if len(tmp)>=2:\n",
    "         if count<ExpectedQuestions:\n",
    "             if tmp[1].isdigit():\n",
    "                 context=int(tmp[1])\n",
    "                 if FoundNumber==1:\n",
    "                    CorrectFormat+=1\n",
    "                    QuestionsAnswers[number]=context\n",
    "         else:\n",
    "             context=' '.join(tmp[1:])\n",
    "             if FoundNumber==1:\n",
    "                CorrectFormat+=1\n",
    "                QuestionsAnswers[number]=context\n",
    "    Correct.append(CorrectFormat==ExpectedQuestions)\n",
    "    Output.append(QuestionsAnswers)\n",
    "  return (Output,Correct)\n",
    "\n",
    "\n",
    "if Customer==1:\n",
    "  (FTOutput,FTCorrectFormat)=FormatOutput(Solutions,StartLine=StartLineFT)\n",
    "elif Customer==0:\n",
    "  (FTOutput,FTCorrectFormat)=FormatOutput(Solutions,StartLine=StartLineFT,ExpectedQuestions=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a434f65-73bb-4525-b256-ae7546178b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 3, 3: 3, 4: 0, 5: 0, 6: 1, 7: 0, 8: 0, 9: 0, 10: 0, 11: 'Customer reports performance issues with their Blockchain as a Service implementation on CDP Private Cloud 7.1.8, including node synchronization failures, slow block validation (>20s), and low throughput (150 TPS vs target 500 TPS) on 3 validator nodes with 16GB RAM each. They are requesting help with identifying bottlenecks and optimization recommendations.'}\n",
      "500\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parsed formatted output for the finetuned LLM\n",
    "print(FTOutput[12])\n",
    "print(len(FTOutput))\n",
    "print(len(FTCorrectFormat))\n",
    "sum(FTCorrectFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9c20632-9d91-42fe-9d68-8b289da67b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if Customer==1:\n",
    "  (BaselineOutput,BaselineCorrectFormat)=FormatOutput(SolutionsBase,StartLine=StartLineBase)\n",
    "elif Customer==0:\n",
    "  (BaselineOutput,BaselineCorrectFormat)=FormatOutput(SolutionsBase,StartLine=StartLineBase,ExpectedQuestions=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d7b136-f25b-4bc8-a34c-c3090e4bd478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 4, 3: 4, 4: 0, 5: 0, 6: 0, 7: 1, 8: 0, 9: 0, 10: 0, 11: \"The comment is from a Cloudera Support Engineer, analyzing a Hive query performance issue in a CDH cluster. The engineer requests the customer to verify and share specific details, including Hive server memory settings, concurrent users, recent data volume changes, and the output of the 'SHOW LOCKS' command. Additionally, the engineer asks the customer to enable query profiling for 24 hours to identify bottlenecks.\"}\n",
      "500\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parsed formatted output for the base LLM\n",
    "\n",
    "print(BaselineOutput[2])\n",
    "print(len(BaselineOutput))\n",
    "print(len(BaselineCorrectFormat))\n",
    "sum(BaselineCorrectFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556a32e-88e3-4902-b60c-67816950e13b",
   "metadata": {},
   "source": [
    "#### **5. LLM-as-a-Judge Evaluation**\n",
    "- Prepares a prompt for LLM-as-a-judge along with three examples to show how to score win rate between the two competing models. \n",
    "- Uses a powerful Phi-4 model to judge which model (finetuned vs baseline) produces better answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8820c54e-4c32-4531-a1b4-7868c8ba51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompts and 3 examples for LLM-as-a-judge to score whether model A or B wins or if there is a tie.\n",
    "\n",
    "\n",
    "comment=\"Hi Support team, we're experiencing critical performance issues with our Hive queries in our production environment. All queries are taking 3-4 times longer than usual to execute, severely impacting our business operations. Can you please escalate this to a high priority case and have someone look into this ASAP? We need immediate assistance as this is affecting our SLAs.\"\n",
    "question=\"\"\"Does this comment relate to a customer complaint? (BOOL)\"\"\"\n",
    "AnswerA=\"\"\"0\"\"\"\n",
    "AnswerB=\"\"\"1\"\"\"\n",
    "\n",
    "Example1Prompt=\"You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + comment +\".\\nHere is the question:\\n\" + question+\":\\n\" +\"\\nHere is the answer of System A:\\n\"+ AnswerA +\"\\nHere is the answer of System B:\\n\"+AnswerB\n",
    "\n",
    "Example1Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"B\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "comment=\"We're experiencing issues with our Cloudera Data Warehouse environment where our Impala queries are failing with OOM errors. The cluster metrics show high memory utilization across all nodes. This is blocking our critical reporting pipeline. Could you please help investigate this issue? We need this resolved within the next 24 hours.\"\n",
    "question=\"\"\"Customer complaint temperature or a frustration level (score 1-4, 0 if not a complaint)\"\"\"\n",
    "AnswerA=\"\"\"3\"\"\"\n",
    "AnswerB=\"\"\"3\"\"\"\n",
    "\n",
    "Example2Prompt=\"You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + comment +\".\\nHere is the question:\\n\" + question+\":\\n\" +\"\\nHere is the answer of System A:\\n\"+ AnswerA +\"\\nHere is the answer of System B:\\n\"+AnswerB\n",
    "\n",
    "Example2Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"tie\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "comment=\"Our Cloudera Manager is showing multiple critical alerts for HDFS data nodes. The replication factor has dropped below the minimum threshold, and we're seeing increased latency in our data processing jobs. Can you please help us resolve this urgently? This is impacting our production workloads.\"\n",
    "question=\"\"\"Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]\"\"\"\n",
    "AnswerA=\"\"\"Critical HDFS issues with data nodes showing alerts, reduced replication factor causing increased latency in production data processing jobs.\"\"\"\n",
    "AnswerB=\"\"\"This is about critical alets.\"\"\"\n",
    "\n",
    "Example3Prompt=\"You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + comment +\".\\nHere is the question:\\n\" + question+\":\\n\" +\"\\nHere is the answer of System A:\\n\"+ AnswerA +\"\\nHere is the answer of System B:\\n\"+AnswerB\n",
    "Example3Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"A\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09e765f0-b4dc-4016-93ca-36db519bd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat2 = [\n",
    "  {\"role\": \"system\", \"content\": System},\n",
    "\n",
    "  {\"role\": \"user\", \"content\": Example1Prompt},\n",
    "  {\"role\": \"assistant\", \"content\": Example1Completion},\n",
    "  {\"role\": \"user\", \"content\": Example2Prompt},\n",
    "  {\"role\": \"assistant\", \"content\": Example2Completion},\n",
    "  {\"role\": \"user\", \"content\": Example3Prompt},\n",
    "  {\"role\": \"assistant\", \"content\": Example3Completion}\n",
    "]\n",
    "\n",
    "Completion=\"\"\"```json\n",
    "{\n",
    "    \"choice\": \"\"\"\n",
    "\n",
    "chat=chat2.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6c95da6-8f65-4251-8810-e4c18d3e2adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "[100264, 9125, 100266, 2675, 527, 264, 11190, 18328, 13, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 13347, 9365, 2128, 11, 584, 2351, 25051, 9200, 5178, 4819, 449, 1057, 69278, 20126, 304, 1057, 5788, 4676, 13, 2052, 20126, 527, 4737, 220, 18, 12, 19, 3115, 5129, 1109, 13783, 311, 9203, 11, 35906, 74055, 1057, 2626, 7677, 13, 3053, 499, 4587, 89690, 420, 311, 264, 1579, 10844, 1162, 323, 617, 4423, 1427, 1139, 420, 67590, 30, 1226, 1205, 14247, 13291, 439, 420, 374, 28987, 1057, 17216, 2170, 35047, 8586, 374, 279, 3488, 512, 22186, 420, 4068, 29243, 311, 264, 6130, 12458, 30, 320, 10611, 7887, 8586, 374, 279, 4320, 315, 744, 362, 512, 15, 198, 8586, 374, 279, 4320, 315, 744, 426, 512, 16, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 330, 33, 702, 534, 74694, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 1687, 2351, 25051, 4819, 449, 1057, 2493, 283, 99003, 2956, 52466, 4676, 1405, 1057, 14727, 6181, 20126, 527, 22109, 449, 507, 1937, 6103, 13, 578, 10879, 17150, 1501, 1579, 5044, 50549, 4028, 682, 7954, 13, 1115, 374, 22978, 1057, 9200, 13122, 15660, 13, 16910, 499, 4587, 1520, 19874, 420, 4360, 30, 1226, 1205, 420, 20250, 2949, 279, 1828, 220, 1187, 4207, 35047, 8586, 374, 279, 3488, 512, 13084, 12458, 9499, 477, 264, 33086, 2237, 320, 12618, 220, 16, 12, 19, 11, 220, 15, 422, 539, 264, 12458, 7887, 8586, 374, 279, 4320, 315, 744, 362, 512, 18, 198, 8586, 374, 279, 4320, 315, 744, 426, 512, 18, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 330, 49831, 702, 534, 74694, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 8140, 2493, 283, 99003, 10790, 374, 9204, 5361, 9200, 30350, 369, 473, 63366, 828, 7954, 13, 578, 48891, 8331, 706, 12504, 3770, 279, 8187, 12447, 11, 323, 584, 2351, 9298, 7319, 40370, 304, 1057, 828, 8863, 7032, 13, 3053, 499, 4587, 1520, 603, 9006, 420, 77720, 30, 1115, 374, 74055, 1057, 5788, 990, 33785, 35047, 8586, 374, 279, 3488, 512, 9370, 5730, 553, 279, 1162, 4068, 9955, 49205, 433, 439, 1790, 439, 3284, 719, 2085, 13490, 3062, 11156, 3649, 13, 507, 1800, 2737, 904, 6574, 22114, 2038, 13, 220, 510, 12998, 69662, 8586, 374, 279, 4320, 315, 744, 362, 512, 43108, 473, 63366, 4819, 449, 828, 7954, 9204, 30350, 11, 11293, 48891, 8331, 14718, 7319, 40370, 304, 5788, 828, 8863, 7032, 627, 8586, 374, 279, 4320, 315, 744, 426, 512, 2028, 374, 922, 9200, 264, 10145, 13, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 330, 32, 702, 534, 74694, 100265, 100264, 882, 100266, 2675, 527, 2728, 264, 4068, 323, 264, 3488, 922, 279, 4068, 13, 1472, 690, 387, 2728, 279, 2077, 315, 1403, 6067, 11, 1887, 362, 323, 1887, 426, 323, 499, 1205, 311, 11913, 902, 1887, 220, 6688, 279, 1888, 2077, 477, 422, 433, 574, 264, 18623, 13, 7557, 2771, 499, 3493, 279, 4320, 304, 3024, 323, 701, 5873, 374, 2728, 304, 264, 2115, 449, 279, 907, 5873, 13, 5810, 374, 279, 4068, 25, 720, 13347, 9365, 8068, 3638, 40, 1120, 4934, 311, 7838, 422, 1070, 374, 264, 16287, 1920, 311, 1715, 1862, 304, 11002, 4221, 13, 1226, 617, 264, 2128, 304, 435, 36834, 31866, 430, 3966, 13291, 449, 1057, 356, 10510, 24047, 13, 7429, 11, 1436, 499, 4587, 7838, 279, 1510, 1862, 4207, 369, 11002, 4221, 1862, 1980, 14809, 24886, 345, 39, 8869, 60521, 627, 8586, 374, 279, 3488, 512, 9370, 5730, 553, 279, 1162, 4068, 9955, 49205, 433, 439, 1790, 439, 3284, 719, 2085, 13490, 3062, 11156, 3649, 13, 507, 1800, 2737, 904, 6574, 22114, 2038, 13, 220, 510, 12998, 60, 720, 8586, 374, 279, 4320, 315, 744, 362, 512, 1687, 1205, 19351, 389, 12743, 9501, 3622, 8670, 369, 264, 2493, 283, 99003, 24047, 304, 279, 4907, 10706, 13, 578, 24047, 690, 1920, 220, 4728, 32260, 315, 7941, 23819, 828, 449, 1972, 7394, 16967, 323, 28975, 3966, 627, 8586, 374, 279, 4320, 315, 744, 426, 512, 13084, 304, 18164, 922, 11002, 4221, 1862, 18539, 323, 1862, 4207, 369, 872, 356, 10510, 24047, 304, 435, 36834, 31866, 13, 2435, 527, 35792, 20109, 315, 279, 1862, 1920, 323, 4221, 1862, 4207, 13, 100265, 100264, 78191, 100266, 74694, 2285, 198, 517, 262, 330, 12008, 794, 220]\n"
     ]
    }
   ],
   "source": [
    "#Setup LLM-as-a-judge final prompt\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(EvalLLM)\n",
    "tokenizer = AutoTokenizer.from_pretrained(EvalLLM)\n",
    "\n",
    "if Customer==1:   \n",
    "    ExpectedQuestions=17\n",
    "\n",
    "elif Customer==0:\n",
    "    ExpectedQuestions=11\n",
    "    \n",
    "FTCorrectFormat={}\n",
    "BaselineCorrectFormat={}\n",
    "AllPrompts=[]\n",
    "\n",
    "for i in range(1,ExpectedQuestions+1):\n",
    "    FTCorrectFormat[i]={}\n",
    "    BaselineCorrectFormat[i]={}\n",
    "    Prompts=[]\n",
    "    print(i)\n",
    "    for j in range(0,len(FTOutput)):\n",
    "        chat=chat2.copy()\n",
    "        \n",
    "        if i in FTOutput[j]:\n",
    "            FTCandidate=str(FTOutput[j][i])\n",
    "            FTCorrectFormat[i][j]=True\n",
    "        else:\n",
    "            FTCandidate=\"\"\n",
    "            FTCorrectFormat[i][j]=False\n",
    "            \n",
    "        if i in BaselineOutput[j]:\n",
    "            BaselineCandidate=str(BaselineOutput[j][i])\n",
    "            BaselineCorrectFormat[i][j]=True\n",
    "        else:\n",
    "            BaselineCandidate=\"\"\n",
    "            BaselineCorrectFormat[i][j]=False\n",
    "        user=\"\"\n",
    "        if j%2==0:\n",
    "          user=\"You are given a comment and a question about the comment. You will be given the response of two systems, system A and system B and you need to judge which system  gave the best response or if it was a tie. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + Comment[j]+\".\\nHere is the question:\\n\"+FixedQuestions[i]+\"\\nHere is the answer of System A:\\n\" + FTCandidate +\"\\nHere is the answer of System B:\\n\"+BaselineCandidate + \"\\n\"\n",
    "        else:\n",
    "          user=\"You are given a comment and a question about the comment. You will be given the response of two systems, system A and system B and you need to judge which system  gave the best response or if it was a tie. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \\n\" + Comment[j]+\".\\nHere is the question:\\n\"+FixedQuestions[i]+\"\\nHere is the answer of System A:\\n\" + BaselineCandidate +\"\\nHere is the answer of System B:\\n\"+FTCandidate + \"\\n\"\n",
    "\n",
    "        chat.append({\"role\": \"user\", \"content\": user})\n",
    "        \n",
    "        Prompt=tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True) + Completion\n",
    "        Prompts.append(Prompt)\n",
    "    AllPrompts.append(Prompts)\n",
    "    \n",
    "\n",
    "\n",
    "print(tokenizer.encode(Prompts[1]))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac083d5-82d9-460f-8d64-6341b2303901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|>You are a helpful assistant.<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "Hi Support team, we're experiencing critical performance issues with our Hive queries in our production environment. All queries are taking 3-4 times longer than usual to execute, severely impacting our business operations. Can you please escalate this to a high priority case and have someone look into this ASAP? We need immediate assistance as this is affecting our SLAs..\n",
      "Here is the question:\n",
      "Does this comment relate to a customer complaint? (BOOL):\n",
      "\n",
      "Here is the answer of System A:\n",
      "0\n",
      "Here is the answer of System B:\n",
      "1<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \"B\"\n",
      "}\n",
      "```\n",
      "<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "We're experiencing issues with our Cloudera Data Warehouse environment where our Impala queries are failing with OOM errors. The cluster metrics show high memory utilization across all nodes. This is blocking our critical reporting pipeline. Could you please help investigate this issue? We need this resolved within the next 24 hours..\n",
      "Here is the question:\n",
      "Customer complaint temperature or a frustration level (score 1-4, 0 if not a complaint):\n",
      "\n",
      "Here is the answer of System A:\n",
      "3\n",
      "Here is the answer of System B:\n",
      "3<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \"tie\"\n",
      "}\n",
      "```\n",
      "<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems and you need to judge which system  gave the best response. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "Our Cloudera Manager is showing multiple critical alerts for HDFS data nodes. The replication factor has dropped below the minimum threshold, and we're seeing increased latency in our data processing jobs. Can you please help us resolve this urgently? This is impacting our production workloads..\n",
      "Here is the question:\n",
      "Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]:\n",
      "\n",
      "Here is the answer of System A:\n",
      "Critical HDFS issues with data nodes showing alerts, reduced replication factor causing increased latency in production data processing jobs.\n",
      "Here is the answer of System B:\n",
      "This is about critical alets.<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \"A\"\n",
      "}\n",
      "```\n",
      "<|im_end|><|im_start|>user<|im_sep|>You are given a comment and a question about the comment. You will be given the response of two systems, system A and system B and you need to judge which system  gave the best response or if it was a tie. Make sure you provide the answer in json and your choice is given in a field with the value choice. Here is the comment: \n",
      "Hi Support Team,\n",
      "\n",
      "We are experiencing issues with concurrent query execution in our CDW Virtual Warehouse. When multiple users (>20) run queries simultaneously, we notice significant query queuing and some queries are getting aborted. Our current setup is:\n",
      "\n",
      "- Medium size warehouse (16-32 cores)\n",
      "- Default resource pool settings\n",
      "- Typical query runtime: 5-10 minutes\n",
      "\n",
      "Could you help us optimize our configuration for better concurrency?\n",
      "\n",
      "Regards,\n",
      "Sarah\n",
      "Data Platform Engineer.\n",
      "Here is the question:\n",
      "Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\n",
      "Here is the answer of System A:\n",
      "1\n",
      "Here is the answer of System B:\n",
      "1\n",
      "<|im_end|><|im_start|>assistant<|im_sep|>```json\n",
      "{\n",
      "    \"choice\": \n"
     ]
    }
   ],
   "source": [
    "#LLM-as-a-judge prompt example\n",
    "print(AllPrompts[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c621694-d7ae-4825-a63a-04fc9abed761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-03 18:39:27 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 04-03 18:39:27 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/phi-4, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 04-03 18:39:30 model_runner.py:1060] Starting to load model microsoft/phi-4...\n",
      "INFO 04-03 18:39:31 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:03,  1.25it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:01<00:03,  1.18it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:02<00:02,  1.21it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:03<00:01,  1.26it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:03<00:00,  1.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.27it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-03 18:39:37 model_runner.py:1071] Loading model weights took 27.3875 GB\n",
      "INFO 04-03 18:39:39 gpu_executor.py:122] # GPU blocks: 702, # CPU blocks: 1310\n",
      "INFO 04-03 18:39:39 gpu_executor.py:126] Maximum concurrency for 10000 tokens per request: 1.12x\n"
     ]
    }
   ],
   "source": [
    "#Load LLM-as-a-judge\n",
    "max_num_seqs=1\n",
    "max_model_len=10000\n",
    "max_num_batched_tokens=10000 \n",
    "gpu_memory_utilization=0.7\n",
    "enforce_eager=True\n",
    "tensor_parallel_size=1\n",
    "seed=None\n",
    "max_tokens=10\n",
    "temperature=0.1\n",
    "\n",
    "import vllm\n",
    "llm = vllm.LLM(model=EvalLLM, max_num_seqs=max_num_seqs,max_model_len=max_model_len,max_num_batched_tokens=max_num_batched_tokens, gpu_memory_utilization=gpu_memory_utilization,enforce_eager=enforce_eager, tensor_parallel_size=tensor_parallel_size)\n",
    "seed=None\n",
    "max_tokens=max_tokens\n",
    "temperature=temperature\n",
    "sampling_params = vllm.SamplingParams(seed=None,max_tokens=max_tokens,temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95152380-07b8-4a73-b991-5fba83efe731",
   "metadata": {},
   "source": [
    "#### **6. Final Metrics**\n",
    "Summarizes the evaluation results, highlighting the finetuned model's performance improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffb2b0ac-8329-4db3-94a3-eb72e5c76a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:49<00:00,  2.94it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 1\n",
      "===================\n",
      "78.23529411764706%\n",
      "21.764705882352942%\n",
      "0.27479338842975204\n",
      "0.07644628099173553\n",
      "133\n",
      "37\n",
      "314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:03<00:00,  2.72it/s, est. speed input: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 2\n",
      "===================\n",
      "45.10739856801909%\n",
      "54.8926014319809%\n",
      "0.3888888888888889\n",
      "0.4732510288065844\n",
      "189\n",
      "230\n",
      "67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [03:01<00:00,  2.76it/s, est. speed input: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 3\n",
      "===================\n",
      "38.57493857493858%\n",
      "61.42506142506142%\n",
      "0.3284518828451883\n",
      "0.5230125523012552\n",
      "157\n",
      "250\n",
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:47<00:00,  2.98it/s, est. speed input: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 4\n",
      "===================\n",
      "85.08771929824562%\n",
      "14.912280701754385%\n",
      "0.20464135021097046\n",
      "0.035864978902953586\n",
      "97\n",
      "17\n",
      "360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.97it/s, est. speed input: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 5\n",
      "===================\n",
      "100.0%\n",
      "0.0%\n",
      "0.022044088176352707\n",
      "0.0\n",
      "11\n",
      "0\n",
      "488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.97it/s, est. speed input: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 6\n",
      "===================\n",
      "97.68211920529801%\n",
      "2.3178807947019866%\n",
      "0.594758064516129\n",
      "0.014112903225806451\n",
      "295\n",
      "7\n",
      "194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:55<00:00,  2.85it/s, est. speed input: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 7\n",
      "===================\n",
      "99.11111111111111%\n",
      "0.8888888888888888%\n",
      "0.48478260869565215\n",
      "0.004347826086956522\n",
      "223\n",
      "2\n",
      "235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.96it/s, est. speed input: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 8\n",
      "===================\n",
      "All outputs are tied\n",
      "0.0\n",
      "0.0\n",
      "0\n",
      "0\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:47<00:00,  2.98it/s, est. speed input: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 9\n",
      "===================\n",
      "100.0%\n",
      "0.0%\n",
      "0.04081632653061224\n",
      "0.0\n",
      "20\n",
      "0\n",
      "470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:48<00:00,  2.97it/s, est. speed input: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 10\n",
      "===================\n",
      "All outputs are tied\n",
      "0.0\n",
      "0.0\n",
      "0\n",
      "0\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 500/500 [02:56<00:00,  2.83it/s, est. speed input: 26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Question 11\n",
      "===================\n",
      "100.0%\n",
      "0.0%\n",
      "1.0\n",
      "0.0\n",
      "500\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Run LLM-as-a-judge for eacb question and at the same time count correct answers/ties.\n",
    "Outs=[]\n",
    "PosPercNoTies=[]\n",
    "NegPercNoTies=[]\n",
    "PosPercTies=[]\n",
    "NegPercTies=[]\n",
    "countPosArr=[]\n",
    "countNegArr=[]\n",
    "countTieArr=[]\n",
    "TiesPerc=[]\n",
    "\n",
    "for j in range(0,ExpectedQuestions):\n",
    "  max_num_seqs=16\n",
    "\n",
    "  sampling_params = vllm.SamplingParams(seed=1,max_tokens=max_tokens,temperature=temperature)\n",
    "\n",
    "  Out=llm.generate(AllPrompts[j], sampling_params)\n",
    "  Outs.append(Out)\n",
    "  countPos=0\n",
    "  countNeg=0\n",
    "  countTie=0\n",
    "\n",
    "  for i in range(0,len(Out)):\n",
    "    if i%2==0:\n",
    "      #print(i)\n",
    "      #print(Out[i].prompt)\n",
    "      #print(Out[i].outputs[0].text[2])\n",
    "      if len(Out[i].outputs[0].text)<2:\n",
    "        continue\n",
    "\n",
    "      if Out[i].outputs[0].text[2]=='A':\n",
    "        countPos+=1\n",
    "      elif Out[i].outputs[0].text[2]=='B':\n",
    "        countNeg+=1\n",
    "      elif Out[i].outputs[0].text[2]=='t':\n",
    "        countTie+=1\n",
    "\n",
    "    if i%2==1:\n",
    "      if len(Out[i].outputs[0].text)<2:\n",
    "        continue\n",
    "\n",
    "      if Out[i].outputs[0].text[2]=='B':\n",
    "        countPos+=1\n",
    "      elif Out[i].outputs[0].text[2]=='A':\n",
    "        countNeg+=1\n",
    "      elif Out[i].outputs[0].text[2]=='t':\n",
    "        countTie+=1\n",
    "  print(\"===================\")\n",
    "  print(\"Question \"+str(j+1))\n",
    "  print(\"===================\")\n",
    "  if countPos+countNeg > 0:\n",
    "    print(str(countPos/(countPos+countNeg)*100)+\"%\")\n",
    "    print(str(countNeg/(countPos+countNeg)*100)+\"%\")\n",
    "  else:\n",
    "      print(\"All outputs are tied\")\n",
    "  print(countPos/(countPos+countNeg+countTie))\n",
    "  print(countNeg/(countPos+countNeg+countTie))\n",
    "\n",
    "  print(countPos)\n",
    "  print(countNeg)\n",
    "  print(countTie)\n",
    "  if countPos+countNeg > 0:\n",
    "    PosPercNoTies.append(str(countPos/(countPos+countNeg)*100)+\"%\")\n",
    "    NegPercNoTies.append(str(countNeg/(countPos+countNeg)*100)+\"%\")\n",
    "  else:\n",
    "    PosPercNoTies.append((\"All outputs are tied\"))\n",
    "    NegPercNoTies.append((\"All outputs are tied\"))\n",
    "\n",
    "  TiesPerc.append(countTie/(countPos+countNeg+countTie))\n",
    "  PosPercTies.append(countPos/(countPos+countNeg+countTie))\n",
    "  NegPercTies.append(countNeg/(countPos+countNeg+countTie))\n",
    "  countPosArr.append(countPos)\n",
    "  countNegArr.append(countNeg)\n",
    "  countTieArr.append(countTie)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6efe9958-6af1-4e88-a15e-36751c90d739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 winrate\n",
      "Does this comment discuss any technical information? (BOOL: 0 for no, 1 for yes)\n",
      "78.23529411764706%\n",
      "----------------------\n",
      "Question 2 winrate\n",
      "Score the severity of the issue based on comment content (SCORE 1-4, lowest=1, highest=4)\n",
      "45.10739856801909%\n",
      "----------------------\n",
      "Question 3 winrate\n",
      "Score the urgency of the issue based on the comment content (SCORE 1-4, lowest=1, highest=4)\n",
      "38.57493857493858%\n",
      "----------------------\n",
      "Question 4 winrate\n",
      "Does the comment have a proposed solution? (BOOL: 0 for no, 1 for yes)\n",
      "85.08771929824562%\n",
      "----------------------\n",
      "Question 5 winrate\n",
      "Does the comment have a proposed workaround?  (BOOL: 0 for no, 1 for yes)\n",
      "100.0%\n",
      "----------------------\n",
      "Question 6 winrate\n",
      "Does the comment have a request for an action from the customer?  (BOOL: 0 for no, 1 for yes)\n",
      "97.68211920529801%\n",
      "----------------------\n",
      "Question 7 winrate\n",
      "Does this comment discuss a bug in Cloudera software? (BOOL: 0 for no, 1 for yes)\n",
      "99.11111111111111%\n",
      "----------------------\n",
      "Question 8 winrate\n",
      "Does the comment include a non-Cloudera Apache JIRA link? (BOOL: 0 for no, 1 for yes)\n",
      "All outputs are tied\n",
      "----------------------\n",
      "Question 9 winrate\n",
      "Does the comment have a link to Cloudera Documentation or Community article? (BOOL: 0 for no, 1 for yes)\n",
      "100.0%\n",
      "----------------------\n",
      "Question 10 winrate\n",
      "Does the comment have any other type of hyperlink? (BOOL: 0 for no, 1 for yes)\n",
      "All outputs are tied\n",
      "----------------------\n",
      "Question 11 winrate\n",
      "Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT] \n",
      "100.0%\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "#Compute winrate\n",
    "counter=1\n",
    "Total=0\n",
    "TotalCounter=0\n",
    "for i in PosPercNoTies:\n",
    "    print(\"Question \"+str(counter)+\" winrate\")\n",
    "    print(FixedQuestions[counter])\n",
    "    print(i)\n",
    "    if i != \"All outputs are tied\":\n",
    "        Total += float(i[:-1])\n",
    "        TotalCounter+=1\n",
    "    counter=counter+1\n",
    "    print('----------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cfaeb05-1d8a-4e09-9e2f-76bd208c7f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "----------------------\n",
      "Average Score\n",
      "82.64428676391772\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "print('----------------------')\n",
    "print('----------------------')\n",
    "print('Average Score')\n",
    "print(Total/TotalCounter)\n",
    "print('----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ebf42-ce86-4e54-816d-24c46ca5b9f4",
   "metadata": {},
   "source": [
    "#### **7. Test if workflow run as expected**\n",
    "- Run with unsloth/Meta-Llama-3.1-8B-Instruct for finetuning\n",
    "- Evaluate using LLM-as-a-judge microsoft/phi-4\n",
    "- Test if the average winrate is approximately 82%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e680e8b4-85f0-43c6-9037-405018b93fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test run as expected.\n"
     ]
    }
   ],
   "source": [
    "if int(Total/TotalCounter) >=81 and int(Total/TotalCounter)<84 :\n",
    "    print(\"Model test run as expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97939e-dbcd-40b1-ae32-fd1ffddbdbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "***If this documentation includes code, including but not limited to, code examples, Cloudera makes this available to you under the terms of the Apache License, Version 2.0, including any required notices.  A copy of the Apache License Version 2.0 can be found [here](https://opensource.org/licenses/Apache-2.0).***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
