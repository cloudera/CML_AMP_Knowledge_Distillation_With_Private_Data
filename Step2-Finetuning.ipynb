{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b6b494-7791-4b17-9ed7-31f153c7b1a2",
   "metadata": {},
   "source": [
    "### **Step2-Finetuning: LoRA Adaptation of Meta-Llama-3.1-8B**\n",
    "\n",
    "#### **1. Configuration Setup**\n",
    "Defines hyperparameters and paths for fine-tuning, including LoRA configuration and hardware settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f51704f-f8b5-46eb-a852-e3048b5759db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-03 18:03:17,422\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "#variables setup for fineuting\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import Libs.DataSynthesisLib as DataSynthesisLib\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "from sys import exit\n",
    "from multiprocess import set_start_method\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "import torch.multiprocessing  as mp\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "GPU_ID=\"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU_ID\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "\n",
    "HasSystem=False\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ModelFT)\n",
    "    chat = [{\"role\": \"system\", \"content\": \"\"}]\n",
    "\n",
    "    tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    HasSystem=True\n",
    "except:\n",
    "    HasSystem=False\n",
    "\n",
    "Config=DataSynthesisLib.GetConfig('Config/Config.py')\n",
    "\n",
    "#Set the model to Finetune\n",
    "Vendor=\"unsloth\"\n",
    "BaseModel=\"Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "QuantModel=BaseModel+\"\"\n",
    "UnQuantModel=BaseModel\n",
    "\n",
    "#Output model path\n",
    "TargetDir=\"tmp/\"\n",
    "TargetDirOutModel=\"tmp/\"\n",
    "\n",
    "UnQuantModelVendor=Vendor+\"/\"+UnQuantModel\n",
    "\n",
    "ModelFT=Vendor+\"/\"+UnQuantModel\n",
    "#Select the data\n",
    "FilenameShort='AllComments_Clean_Train'\n",
    "Filename='Data/AllComments_Clean_Train'\n",
    "\n",
    "with open(Filename + \".json\", \"r\") as f:\n",
    "    TrainData=json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6df2593-433f-4fc6-aa29-3589d550c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finetuning parameters\n",
    "Data_Size=5000\n",
    "#########CHange per model\n",
    "AssistantToken=\"<|start_header_id|>assistant\"\n",
    "TrimBeg=1\n",
    "Filename2=Filename+\"_Datasize_\"+str(Data_Size) +'_'+BaseModel\n",
    "ConfigS=Config[\"ConfigSolution\"].copy()\n",
    "ConfigS[\"lora_r\"]=128\n",
    "ConfigS[\"target_modules\"]=['q_proj','k_proj','v_proj','o_proj']\n",
    "ConfigS[\"target_modules\"]=\"all-linear\"\n",
    "ConfigS[\"lora_alpha\"]=64\n",
    "ConfigS[\"lora_dropout\"]=0.05\n",
    "\n",
    "ConfigS[\"gpu-id\"]=GPU_ID\n",
    "\n",
    "ConfigS[\"max_model_len\"]=\"8000\"\n",
    "ConfigS[\"max_num_batched_tokens\"]=\"8000\"\n",
    "ConfigS[\"llm-path\"]=ModelFT\n",
    "ConfigS[\"tokenizer-path\"]=ModelFT\n",
    "\n",
    "ConfigS[\"tokenizer-file-gguf\"]=\"\"\n",
    "ConfigS[\"tensor_parallel_size\"]=\"1\"\n",
    "ConfigS[\"use-grammar\"]=\"False\"\n",
    "ConfigS[\"BlockSize\"]=\"400\"\n",
    "ConfigS[\"FT-num_train_epochs\"]=\"1\"\n",
    "\n",
    "ConfigS[\"GenerateChatTemplate\"]=\"GenerateChatWithSystemWithAssistant\"\n",
    "ConfigS[\"UseOutlines\"]=\"False\"\n",
    "ConfigS[\"FT-SaveModel\"]=TargetDir+Filename2\n",
    "ConfigS[\"FT-SaveModel\"]=TargetDir+\"all-linear\"+FilenameShort+\"r_\"+str(ConfigS[\"lora_r\"])+'_'+'a_'+str(ConfigS[\"lora_alpha\"])+'_'+'d_'+str(ConfigS[\"lora_dropout\"])\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ConfigS[\"gpu-id\"]\n",
    "os.environ[\"NCCL_SHM_DISABLE\"]=\"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d3e19d-2d0a-4fb0-80de-35da5deac91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a customer comment, 17 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the customer comment:\n",
      "\n",
      "Hi Cloudera Support,\n",
      "\n",
      "Our Sentry service in the CDH cluster has stopped responding, and users are unable to access Hive tables and Impala queries are failing with permission errors. We've tried restarting the Sentry service multiple times, but it keeps crashing after a few minutes. The logs show multiple Java heap space errors. This is blocking our entire data analytics team from accessing any data.\n",
      "\n",
      "Best regards,\n",
      "Robert\n",
      "\n",
      "Here are the 17 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Does this comment relate to a customer complaint? (answer 0 for no, 1 for yes)\n",
      "3. Customer complaint temperature or a frustration level (if there is a complain give 1 for lowest, 4 for highest and 2,3 for in between. If there is no complain give a score of 0).\n",
      "4. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "5. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "6. Is this a request from a customer for an update? (answer 0 for no, 1 for yes)\n",
      "7. Is there a strictly explicit and NOT an implied request from a customer for a call, meeting or a screenshare (zoom/webex/teams etc.)? Do not answer yes unless wording explicitly asks for a call. ((BOOL:0/1)\n",
      "8. Did the customer request an escalation? (answer 0 for no, 1 for yes)\n",
      "9. Did the customer request a priority change?  To what level? (If there is a priority change give score 1 to indicate highest priority (indicated by S1) and 4  to indicate the lowest priority (Indicated by S4). If there is no priority change give a score of 0).\n",
      "10. Did the customer request a transfer to another Customer Operations Engineer? (answer 0 for no, 1 for yes)\n",
      "11. Did the customer request to speak to a manager or supervisor? (answer 0 for no, 1 for yes)\n",
      "12. Did the customer request a Subject Matter Expert or expert? (answer 0 for no, 1 for yes)\n",
      "13. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "14. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "15. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "16. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "17. Summarize the case comment condensing itas much as possible but without losing important technical details. Omit including any meeting invite information. (TEXT)\n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n",
      "--------------------\n",
      "You are given a Cloudera support team comment, 11 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the Cloudera comment:\n",
      "    \n",
      "Hi Support Team,\n",
      "\n",
      "We are experiencing unusual behavior in our Spark application where the stragglers are taking significantly longer to complete. The logs show:\n",
      "\n",
      "WARN TaskSetManager: Stage 2 contains tasks with high variance in duration. Top 3 tasks took 450s, 425s, 380s while median is 120s.\n",
      "\n",
      "Our configuration:\n",
      "spark.speculation=false\n",
      "spark.executor.cores=4\n",
      "spark.executor.memory=8g\n",
      "\n",
      "Can you help us optimize the configuration to improve performance?\n",
      "\n",
      "Thanks,\n",
      "Alex\n",
      "\n",
      "Here are the 11 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "3. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "4. Does the comment have a proposed solution? (answer 0 for no, 1 for yes)\n",
      "5. Does the comment have a proposed workaround?  (answer 0 for no, 1 for yes)\n",
      "6. Does the comment have a request for an action from the customer?  (answer 0 for no, 1 for yes)\n",
      "7. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "8. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "9. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "10. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "11. Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]    \n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training data examples\n",
    "print(TrainData[0]['Prompt'])\n",
    "print('--------------------')\n",
    "print(TrainData[1]['Prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c618f-b7fe-41b8-b581-64a37c7ce16d",
   "metadata": {},
   "source": [
    "#### **2. Data Formatting for Finetuning**\n",
    "Converts the training data into chat templates compatible with the LLM's expected input format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8efb36-3f4c-4afa-93c3-105747531efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert input training data to chat completion\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(ModelFT)\n",
    "\n",
    "# Helper function to format the dataset\n",
    "def chatml_format(Data,tokenizer,Data_Size):\n",
    "  Out=[]\n",
    "  OutFormat2={\n",
    "        \"text\": []\n",
    "  }\n",
    "  for i in range(0,min(len(Data['Questions']),Data_Size)):\n",
    "    #if Data['UnitTestsPass'][i]==True:\n",
    "      System={\"role\": \"system\",    \"content\": Data['system']}\n",
    "      if HasSystem==True:\n",
    "        message = [System,\n",
    "                 {\"role\": \"user\",      \"content\": Data['Questions'][i]},\n",
    "                 {\"role\": \"assistant\", \"content\": Data[\"SolutionsCleanedPython\"][i]}]\n",
    "      else:\n",
    "        message = [\n",
    "                 {\"role\": \"user\",      \"content\": Data['Questions'][i]},\n",
    "                 {\"role\": \"assistant\", \"content\": Data[\"SolutionsCleanedPython\"][i]}]\n",
    "\n",
    "      system_user_completion=tokenizer.apply_chat_template(message, tokenize=False,add_generation_prompt=False)\n",
    "\n",
    "      OutFormat2[\"text\"].append(system_user_completion)\n",
    "      d={}\n",
    "      d['text']=system_user_completion\n",
    "      Out.append(d)\n",
    "  return (OutFormat2,Out)\n",
    "\n",
    "MyDatasetTrain=TrainData.copy()\n",
    "\n",
    "MyDatasetTrain={}\n",
    "MyDatasetTrain[\"system\"]='You are a helpful assistant.'\n",
    "MyDatasetTrain[\"Questions\"]=[i[\"Prompt\"] for i in TrainData]\n",
    "MyDatasetTrain[\"SolutionsCleanedPython\"]=[i[\"Completion\"] for i in TrainData]\n",
    "\n",
    "\n",
    "(MyDatasetTrainSFT,MyDatasetTrainSFTJson)=chatml_format(MyDatasetTrain,tokenizer,Data_Size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae7699d-a071-474e-a3ea-6700e0cea947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are given a Cloudera support team comment, 11 questions referring to the comment, and the possible values for each question in parenthesis following the question. Here is the Cloudera comment:\n",
      "    \n",
      "Hi Support Team,\n",
      "\n",
      "We are experiencing unusual behavior in our Spark application where the stragglers are taking significantly longer to complete. The logs show:\n",
      "\n",
      "WARN TaskSetManager: Stage 2 contains tasks with high variance in duration. Top 3 tasks took 450s, 425s, 380s while median is 120s.\n",
      "\n",
      "Our configuration:\n",
      "spark.speculation=false\n",
      "spark.executor.cores=4\n",
      "spark.executor.memory=8g\n",
      "\n",
      "Can you help us optimize the configuration to improve performance?\n",
      "\n",
      "Thanks,\n",
      "Alex\n",
      "\n",
      "Here are the 11 questions:\n",
      "1. Does this comment discuss any technical information? (answer 0 for no, 1 for yes)\n",
      "2. Score the severity of the issue based on comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "3. Score the urgency of the issue based on the comment content (SCORE 1-4, give 1 for lowest, 4 for highest and 2,3 for in between)\n",
      "4. Does the comment have a proposed solution? (answer 0 for no, 1 for yes)\n",
      "5. Does the comment have a proposed workaround?  (answer 0 for no, 1 for yes)\n",
      "6. Does the comment have a request for an action from the customer?  (answer 0 for no, 1 for yes)\n",
      "7. Does this comment discuss a bug in Cloudera software? (answer 0 for no, 1 for yes)\n",
      "8. Does the comment include a non-Cloudera Apache JIRA link (e.g. a Apache JIRA link with issues.apach.org domain name)? (answer 0 for no, 1 for yes)\n",
      "9. Does the comment have a link to Cloudera Documentation or Community article? (answer 0 for no, 1 for yes)\n",
      "10. Does the comment have any other type of hyperlink? (answer 0 for no, 1 for yes)\n",
      "11. Summarize the case comment condensing it as much as possible but without losing important technical details. Omit including any meeting invite information.  [TEXT]    \n",
      "Generate each answer in a new line using the the format \"Number. answer\" (e.g. 1. 0)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1. 1\n",
      "2. 2\n",
      "3. 2\n",
      "4. 0\n",
      "5. 0\n",
      "6. 1\n",
      "7. 0\n",
      "8. 0\n",
      "9. 0\n",
      "10. 0\n",
      "11. Alex reports performance issues in their Spark application where some tasks (stragglers) are taking significantly longer to complete compared to the median task duration. The logs show high variance in task duration with top tasks taking 450s, 425s, and 380s while the median is 120s. Current configuration includes speculation disabled and specific executor settings.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(MyDatasetTrainSFT['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee51aa12-0595-4b6e-9a36-0a06128c1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into dev/training dataset (90% training)\n",
    "\n",
    "ds_len=len(MyDatasetTrainSFT['text'])\n",
    "df_test = pd.DataFrame.from_dict(MyDatasetTrainSFT)[9*ds_len//10:]\n",
    "df_train = pd.DataFrame.from_dict(MyDatasetTrainSFT)[:9*ds_len//10]\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b10336-ddc9-4628-89ac-a0d7fdc77b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4500\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14f242-8983-44d1-aec0-229e25479133",
   "metadata": {},
   "source": [
    "#### **3. Training Execution**\n",
    "Configures the LLM for fine-tuning, initializes LoRA adapters, and trains the model. Uses multiprocessing to avoid CUDA memory leaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9165477a-83a1-4c17-b919-a72162fd7814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.18s/it]\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_529/525853314.py:65: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Map: 100%|█████████████████████████| 4500/4500 [00:01<00:00, 3304.35 examples/s]\n",
      "Map: 100%|███████████████████████████| 500/500 [00:00<00:00, 3353.57 examples/s]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "No experiment set using default experiment.Please set experiment using mlflow.set_experiment('<your experiment name>') to avoid using default experiment.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='562' max='562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [562/562 27:30, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.267200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.218500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#Do finetuning\n",
    "\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "def SFTTrain(ConfigS,Input):\n",
    "  \n",
    "  import os\n",
    "  os.environ[\"CUDA_VISIBLE_DEVICES\"]=ConfigS[\"gpu-id\"]\n",
    "  from peft import LoraModel, LoraConfig\n",
    "  import peft\n",
    "  import torch\n",
    "  from trl import SFTConfig, SFTTrainer\n",
    "  from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "  from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForSeq2Seq #,infer_auto_device_map\n",
    "  from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "  from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "  device_map={'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 0, 'model.layers.25': 0, 'model.layers.26': 0, 'model.layers.27': 0, 'model.layers.28': 0, 'model.layers.29': 0, 'model.layers.30': 0, 'model.layers.31': 0, 'model.layers.32': 0, 'model.layers.33': 0, 'model.layers.34': 0, 'model.layers.35': 0, 'model.layers.36': 0, 'model.layers.37': 0, 'model.layers.38': 0, 'model.layers.39': 0, 'model.layers.40': 0, 'model.layers.41': 0, 'model.layers.42': 0, 'model.layers.43': 0, 'model.layers.44': 0, 'model.layers.45': 1, 'model.layers.46': 1, 'model.layers.47': 1, 'model.layers.48': 1, 'model.layers.49': 1, 'model.layers.50': 1, 'model.layers.51': 1, 'model.layers.52': 1, 'model.layers.53': 1, 'model.layers.54': 1, 'model.layers.55': 1, 'model.layers.56': 1, 'model.layers.57': 1, 'model.layers.58': 1, 'model.layers.59': 1, 'model.layers.60': 1, 'model.layers.61': 1, 'model.layers.62': 1, 'model.layers.63': 1, 'model.norm': 1, 'model.rotary_emb': 0, 'lm_head': 1}\n",
    "  dataset_train=Input[\"dataset_train\"]\n",
    "  dataset_test=Input[\"dataset_test\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  model=AutoModelForCausalLM.from_pretrained(ConfigS[\"llm-path\"], device_map=\"cuda\",torch_dtype=torch.float16)\n",
    "  \n",
    "            \n",
    "  tokenizer = AutoTokenizer. from_pretrained(ConfigS[\"llm-path\"])\n",
    "\n",
    "  model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "\n",
    "  peft_config = LoraConfig(\n",
    "    r=ConfigS[\"lora_r\"],\n",
    "    lora_alpha=ConfigS[\"lora_alpha\"],\n",
    "    target_modules=ConfigS[\"target_modules\"],\n",
    "    lora_dropout=ConfigS[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "  )\n",
    "  model.enable_input_require_grads() \n",
    "  model = peft.get_peft_model(model, peft_config)\n",
    "  optimizer=torch.optim.AdamW(model.parameters(),float(ConfigS[\"FT-learning_rate\"]))\n",
    "  training_args = SFTConfig(   \n",
    "    max_seq_length=4096,\n",
    "    output_dir=TargetDir+ConfigS[\"FT-output-dir\"],\n",
    "    do_train=bool(ConfigS[\"FT-do_train\"]),\n",
    "    do_eval=bool(ConfigS[\"FT-do_eval\"]),\n",
    "    per_device_train_batch_size=int(ConfigS[\"FT-per_device_train_batch_size\"]),\n",
    "    per_device_eval_batch_size=int(ConfigS[\"FT-per_device_eval_batch_size\"]),\n",
    "    learning_rate=float(ConfigS[\"FT-learning_rate\"]),\n",
    "    num_train_epochs=int(ConfigS[\"FT-num_train_epochs\"]),\n",
    "    logging_dir=ConfigS[\"FT-logging_dir\"],\n",
    "    logging_steps=int(ConfigS[\"FT-logging_steps\"]),\n",
    "    save_steps=int(ConfigS[\"FT-save_steps\"]),\n",
    "    optim=ConfigS[\"FT-optim\"],\n",
    "\n",
    "    dataset_text_field='text',\n",
    "\n",
    "    gradient_accumulation_steps=int(ConfigS[\"gradient_accumulation_steps\"]),\n",
    "    eval_steps=10,\n",
    "    fp16=True\n",
    "  )\n",
    "\n",
    "  \n",
    "  trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer,None)\n",
    "  )\n",
    "  res=trainer.train()\n",
    "\n",
    "  trainer.save_model(ConfigS[\"FT-SaveModel\"])\n",
    "    \n",
    "  return res\n",
    "\n",
    "Input={}\n",
    "Input[\"dataset_train\"]=dataset_train\n",
    "Input[\"dataset_test\"]=dataset_test\n",
    "#Launch finetuning\n",
    "p=mp.Process(target=SFTTrain,args=(ConfigS,Input))\n",
    "p.start()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf7e6bf9-7e2d-46fe-a6dd-08898e4f14c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/Meta-Llama-3.1-8B-Instruct\n",
      "tmp/merged_AllComments_Clean_Trainr_128_a_64_d_0.05\n"
     ]
    }
   ],
   "source": [
    "TargetModel=TargetDirOutModel+'merged_'+FilenameShort+\"r_\"+str(ConfigS[\"lora_r\"])+'_'+'a_'+str(ConfigS[\"lora_alpha\"])+'_'+'d_'+str(ConfigS[\"lora_dropout\"])\n",
    "print(ModelFT)\n",
    "print(TargetModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856696cc-9a05-4236-b696-661a10a8050c",
   "metadata": {},
   "source": [
    "#### **4. Merge Adapters into Final Model**\n",
    "Combines the base model with LoRA adapters to create a single deployable model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7514f4-8271-4c32-952f-4943e720b6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "#Merge model and adapters\n",
    "def MergeAndSave(ModelBase, ModelAdapter, OutputModel):\n",
    "  tokenizer=AutoTokenizer.from_pretrained(ModelBase)\n",
    "\n",
    "  base_model = AutoModelForCausalLM.from_pretrained(ModelBase,torch_dtype=torch.float16)\n",
    "\n",
    "  peft_model_id = ModelAdapter\n",
    "  model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "  merged_model = model.merge_and_unload()\n",
    "\n",
    "  merged_model.save_pretrained(OutputModel)\n",
    "  tokenizer.save_pretrained(OutputModel)\n",
    "\n",
    "p=mp.Process(target=MergeAndSave,args=(ModelFT, ConfigS[\"FT-SaveModel\"], TargetModel))\n",
    "p.start()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf33e5-939e-4fed-878d-f56d31cccaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "***If this documentation includes code, including but not limited to, code examples, Cloudera makes this available to you under the terms of the Apache License, Version 2.0, including any required notices.  A copy of the Apache License Version 2.0 can be found [here](https://opensource.org/licenses/Apache-2.0).***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
